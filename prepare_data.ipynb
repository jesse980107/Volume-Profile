{
 "cells": [
  {
   "cell_type": "code",
   "source": "# ä¿å­˜ä¸ºé¡¹ç›®æ‰€éœ€æ ¼å¼ï¼š300809.sz.csv\n# æ³¨æ„ï¼šæ–‡ä»¶åä½¿ç”¨å°å†™ sz\n\n# æå–è‚¡ç¥¨ä»£ç çš„æ•°å­—å’Œåç¼€éƒ¨åˆ†\ncode_number = TS_CODE.split('.')[0]  # '300809'\ncode_suffix = TS_CODE.split('.')[1].lower()  # 'sz'\n\n# æ„å»ºæ–‡ä»¶å\noutput_filename = f'data/{code_number}.{code_suffix}.csv'\n\n# ä¿å­˜åˆå¹¶åçš„æ•°æ®\ndf_merged.to_csv(output_filename, index=False, encoding='utf-8-sig')\n\nprint(\"=\"*60)\nprint(f\"âœ… é¡¹ç›®æ ¼å¼æ•°æ®å·²ä¿å­˜: {output_filename}\")\nprint(f\"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_filename) / 1024:.2f} KB\")\nprint(f\"   è®°å½•æ•°: {len(df_merged)}\")\nprint(f\"   æ—¥æœŸèŒƒå›´: {df_merged['trade_date'].min()} ~ {df_merged['trade_date'].max()}\")\nprint(\"=\"*60)\nprint(\"\\nğŸ‰ ç°åœ¨å¯ä»¥åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¯¥è‚¡ç¥¨æ•°æ®äº†ï¼\")\nprint(f\"   è‚¡ç¥¨ä»£ç : {TS_CODE}\")\nprint(f\"   æ•°æ®æ–‡ä»¶: {output_filename}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. ä¿å­˜ä¸ºé¡¹ç›®æ‰€éœ€æ ¼å¼ï¼ˆ300809.sz.csvï¼‰",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 000155 è‚¡ç¥¨æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "æœ¬ Notebook ç”¨äºä» Tushare æ‹‰å– 000155ï¼ˆä¸­è”é‡ç§‘ï¼‰çš„**å…¨éƒ¨å†å²æ—¥çº¿æ•°æ®**å’Œ **daily_basic æ•°æ®**ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥ä¾èµ–åŒ…\n",
    "\n",
    "```bash\n",
    "pip install tushare pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(f\"tushare version: {ts.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. é…ç½® Tushare Token\n",
    "\n",
    "**è¯·æ›¿æ¢ä¸ºä½ çš„ Tushare Pro Token**\n",
    "\n",
    "è·å–æ–¹å¼ï¼šhttps://tushare.pro/register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# âš ï¸ è¯·æ›¿æ¢ä¸ºä½ çš„ Token\nTOKEN = 'your_tushare_token_here'\n\n# åˆå§‹åŒ– Tushare Pro API\npro = ts.pro_api(TOKEN)\n\n# è‚¡ç¥¨ä»£ç \nTS_CODE = '000155.SZ'  # ä¸­è”é‡ç§‘\n\n# ä½¿ç”¨é€šç”¨çš„æ—©æœŸæ—¥æœŸï¼Œæ— éœ€æ‰‹åŠ¨æŸ¥æ‰¾ä¸Šå¸‚æ—¥æœŸ\n# 1990-01-01 æ—©äºæ‰€æœ‰Aè‚¡ä¸Šå¸‚æ—¥æœŸï¼ŒAPIä¼šè‡ªåŠ¨ä»å®é™…ä¸Šå¸‚æ—¥å¼€å§‹è¿”å›æ•°æ®\nSTART_DATE = '19900101'\n\nprint(f\"è‚¡ç¥¨ä»£ç : {TS_CODE}\")\nprint(f\"æŸ¥è¯¢èµ·å§‹æ—¥æœŸ: {START_DATE}ï¼ˆå®é™…ä»ä¸Šå¸‚æ—¥å¼€å§‹ï¼‰\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å®šä¹‰æ•°æ®è·å–å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_daily_data(ts_code, start_date, sleep_time=0.3):\n",
    "    \"\"\"\n",
    "    åˆ†æ‰¹è·å–å…¨éƒ¨å†å²æ—¥çº¿æ•°æ®\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ts_code : str\n",
    "        è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '000155.SZ'\n",
    "    start_date : str\n",
    "        å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'\n",
    "    sleep_time : float\n",
    "        æ¯æ¬¡è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å…è¯·æ±‚è¿‡å¿«\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        åŒ…å«å…¨éƒ¨å†å²æ—¥çº¿æ•°æ®çš„ DataFrame\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # è®¡ç®—æ—¥æœŸèŒƒå›´\n",
    "    start = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end = datetime.now()\n",
    "    \n",
    "    current_start = start\n",
    "    \n",
    "    print(f\"å¼€å§‹è·å–æ—¥çº¿æ•°æ®: {start_date} ~ {end.strftime('%Y%m%d')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    batch_count = 0\n",
    "    \n",
    "    # æ¯æ¬¡è·å–1å¹´çš„æ•°æ®\n",
    "    while current_start < end:\n",
    "        current_end = min(current_start + timedelta(days=365), end)\n",
    "        \n",
    "        start_str = current_start.strftime('%Y%m%d')\n",
    "        end_str = current_end.strftime('%Y%m%d')\n",
    "        \n",
    "        try:\n",
    "            df = pro.daily(\n",
    "                ts_code=ts_code,\n",
    "                start_date=start_str,\n",
    "                end_date=end_str\n",
    "            )\n",
    "            \n",
    "            if not df.empty:\n",
    "                all_data.append(df)\n",
    "                batch_count += 1\n",
    "                print(f\"[{batch_count:2d}] {start_str} ~ {end_str}: {len(df):4d} æ¡\")\n",
    "            else:\n",
    "                print(f\"[{batch_count:2d}] {start_str} ~ {end_str}: æ— æ•°æ®\")\n",
    "            \n",
    "            time.sleep(sleep_time)  # é¿å…è¯·æ±‚è¿‡å¿«\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è·å–æ•°æ®å¤±è´¥ [{start_str} ~ {end_str}]: {e}\")\n",
    "        \n",
    "        current_start = current_end + timedelta(days=1)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰æ•°æ®\n",
    "    if all_data:\n",
    "        result = pd.concat(all_data, ignore_index=True)\n",
    "        # å»é‡å¹¶æ’åº\n",
    "        result = result.drop_duplicates(subset=['trade_date']).sort_values('trade_date')\n",
    "        print(f\"âœ… æ—¥çº¿æ•°æ®è·å–å®Œæˆï¼Œå…± {len(result)} æ¡è®°å½•\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_all_daily_basic(ts_code, start_date, sleep_time=0.3):\n    \"\"\"\n    åˆ†æ‰¹è·å–å…¨éƒ¨å†å² daily_basic æ•°æ®ï¼ˆåŒ…å«æµé€šè‚¡æœ¬ç­‰æŒ‡æ ‡ï¼‰\n    \n    Parameters:\n    -----------\n    ts_code : str\n        è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '000155.SZ'\n    start_date : str\n        å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'\n    sleep_time : float\n        æ¯æ¬¡è¯·æ±‚é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰\n    \n    Returns:\n    --------\n    pd.DataFrame\n        åŒ…å«å…¨éƒ¨å†å² daily_basic æ•°æ®çš„ DataFrame\n    \"\"\"\n    all_data = []\n    \n    start = datetime.strptime(start_date, '%Y%m%d')\n    end = datetime.now()\n    \n    current_start = start\n    \n    print(f\"å¼€å§‹è·å– daily_basic æ•°æ®: {start_date} ~ {end.strftime('%Y%m%d')}\")\n    print(\"=\" * 60)\n    \n    batch_count = 0\n    \n    while current_start < end:\n        current_end = min(current_start + timedelta(days=365), end)\n        \n        start_str = current_start.strftime('%Y%m%d')\n        end_str = current_end.strftime('%Y%m%d')\n        \n        try:\n            df = pro.daily_basic(\n                ts_code=ts_code,\n                start_date=start_str,\n                end_date=end_str,\n                # æ³¨æ„ï¼šç§»é™¤äº† close å­—æ®µï¼Œå› ä¸º daily æ¥å£å·²ç»æä¾›äº†\n                fields='ts_code,trade_date,turnover_rate,turnover_rate_f,volume_ratio,pe,pe_ttm,pb,ps,ps_ttm,dv_ratio,dv_ttm,total_share,float_share,free_share,total_mv,circ_mv'\n            )\n            \n            if not df.empty:\n                all_data.append(df)\n                batch_count += 1\n                print(f\"[{batch_count:2d}] {start_str} ~ {end_str}: {len(df):4d} æ¡\")\n            else:\n                print(f\"[{batch_count:2d}] {start_str} ~ {end_str}: æ— æ•°æ®\")\n            \n            time.sleep(sleep_time)\n            \n        except Exception as e:\n            print(f\"âŒ è·å–æ•°æ®å¤±è´¥ [{start_str} ~ {end_str}]: {e}\")\n        \n        current_start = current_end + timedelta(days=1)\n    \n    print(\"=\" * 60)\n    \n    if all_data:\n        result = pd.concat(all_data, ignore_index=True)\n        result = result.drop_duplicates(subset=['trade_date']).sort_values('trade_date')\n        print(f\"âœ… daily_basic æ•°æ®è·å–å®Œæˆï¼Œå…± {len(result)} æ¡è®°å½•\")\n        return result\n    else:\n        print(\"âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®\")\n        return pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. è·å–æ—¥çº¿æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è·å–å…¨éƒ¨å†å²æ—¥çº¿æ•°æ®\ndf_daily = get_all_daily_data(TS_CODE, START_DATE, sleep_time=0.3)\n\n# æŸ¥çœ‹æ•°æ®\nprint(\"\\næ•°æ®é¢„è§ˆ:\")\ndisplay(df_daily.head())\nprint(\"\\næ•°æ®ä¿¡æ¯:\")\nprint(df_daily.info())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. è·å– daily_basic æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# è·å–å…¨éƒ¨å†å² daily_basic æ•°æ®\ndf_basic = get_all_daily_basic(TS_CODE, START_DATE, sleep_time=0.3)\n\n# æŸ¥çœ‹æ•°æ®\nprint(\"\\næ•°æ®é¢„è§ˆ:\")\ndisplay(df_basic.head())\nprint(\"\\næ•°æ®ä¿¡æ¯:\")\nprint(df_basic.info())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. åˆå¹¶æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶æ—¥çº¿æ•°æ®å’Œ basic æ•°æ®\n",
    "df_merged = pd.merge(\n",
    "    df_daily, \n",
    "    df_basic, \n",
    "    on=['ts_code', 'trade_date'], \n",
    "    how='left',\n",
    "    suffixes=('', '_basic')\n",
    ")\n",
    "\n",
    "print(f\"âœ… æ•°æ®åˆå¹¶å®Œæˆ\")\n",
    "print(f\"   - æ—¥çº¿æ•°æ®: {len(df_daily)} æ¡\")\n",
    "print(f\"   - basicæ•°æ®: {len(df_basic)} æ¡\")\n",
    "print(f\"   - åˆå¹¶å: {len(df_merged)} æ¡\")\n",
    "\n",
    "# æŸ¥çœ‹åˆå¹¶åçš„æ•°æ®\n",
    "print(\"\\nåˆå¹¶åæ•°æ®é¢„è§ˆ:\")\n",
    "display(df_merged.head())\n",
    "\n",
    "# æŸ¥çœ‹åˆ—å\n",
    "print(\"\\nåˆ—å:\")\n",
    "print(df_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ•°æ®è´¨é‡æ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"æ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åŸºæœ¬ç»Ÿè®¡\n",
    "print(f\"\\n1. åŸºæœ¬ä¿¡æ¯\")\n",
    "print(f\"   è‚¡ç¥¨ä»£ç : {TS_CODE}\")\n",
    "print(f\"   æ•°æ®èµ·å§‹: {df_merged['trade_date'].min()}\")\n",
    "print(f\"   æ•°æ®ç»ˆæ­¢: {df_merged['trade_date'].max()}\")\n",
    "print(f\"   æ€»è®°å½•æ•°: {len(df_merged)}\")\n",
    "\n",
    "# ç¼ºå¤±å€¼æ£€æŸ¥\n",
    "print(f\"\\n2. ç¼ºå¤±å€¼ç»Ÿè®¡\")\n",
    "missing = df_merged.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if len(missing) > 0:\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"   âœ… æ— ç¼ºå¤±å€¼\")\n",
    "\n",
    "# é‡å¤å€¼æ£€æŸ¥\n",
    "print(f\"\\n3. é‡å¤å€¼æ£€æŸ¥\")\n",
    "duplicates = df_merged.duplicated(subset=['trade_date']).sum()\n",
    "print(f\"   é‡å¤æ—¥æœŸæ•°: {duplicates}\")\n",
    "\n",
    "# æ•°æ®èŒƒå›´\n",
    "print(f\"\\n4. ä»·æ ¼æ•°æ®ç»Ÿè®¡\")\n",
    "print(f\"   æœ€é«˜ä»·: {df_merged['high'].max():.2f}\")\n",
    "print(f\"   æœ€ä½ä»·: {df_merged['low'].min():.2f}\")\n",
    "print(f\"   å¹³å‡æ”¶ç›˜ä»·: {df_merged['close'].mean():.2f}\")\n",
    "\n",
    "# æˆäº¤é‡ç»Ÿè®¡\n",
    "print(f\"\\n5. æˆäº¤é‡ç»Ÿè®¡\")\n",
    "print(f\"   å¹³å‡æˆäº¤é‡: {df_merged['vol'].mean()/10000:.2f} ä¸‡æ‰‹\")\n",
    "print(f\"   æœ€å¤§æˆäº¤é‡: {df_merged['vol'].max()/10000:.2f} ä¸‡æ‰‹\")\n",
    "\n",
    "# è‚¡æœ¬ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "if 'total_share' in df_merged.columns:\n",
    "    print(f\"\\n6. è‚¡æœ¬ç»Ÿè®¡ï¼ˆæœ€æ–°ï¼‰\")\n",
    "    latest = df_merged.iloc[-1]\n",
    "    print(f\"   æ€»è‚¡æœ¬: {latest['total_share']:.2f} ä¸‡è‚¡\")\n",
    "    if pd.notna(latest.get('float_share')):\n",
    "        print(f\"   æµé€šè‚¡æœ¬: {latest['float_share']:.2f} ä¸‡è‚¡\")\n",
    "    if pd.notna(latest.get('free_share')):\n",
    "        print(f\"   è‡ªç”±æµé€šè‚¡æœ¬: {latest['free_share']:.2f} ä¸‡è‚¡\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ä¿å­˜æ•°æ®åˆ°æœ¬åœ°\n",
    "\n",
    "å°†æ•°æ®ä¿å­˜ä¸ºå¤šç§æ ¼å¼ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»º data ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# æ–‡ä»¶è·¯å¾„\n",
    "csv_path = f'data/{TS_CODE.replace(\".\", \"_\")}_full_history.csv'\n",
    "csv_daily_path = f'data/{TS_CODE.replace(\".\", \"_\")}_daily.csv'\n",
    "csv_basic_path = f'data/{TS_CODE.replace(\".\", \"_\")}_basic.csv'\n",
    "\n",
    "# ä¿å­˜åˆå¹¶æ•°æ®\n",
    "print(\"å¼€å§‹ä¿å­˜æ•°æ®...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ•°æ®\n",
    "df_merged.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"âœ… åˆå¹¶æ•°æ®å·²ä¿å­˜: {csv_path}\")\n",
    "print(f\"   æ–‡ä»¶å¤§å°: {os.path.getsize(csv_path) / 1024:.2f} KB\")\n",
    "\n",
    "# 2. åˆ†åˆ«ä¿å­˜æ—¥çº¿å’Œ basic æ•°æ®\n",
    "df_daily.to_csv(csv_daily_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… æ—¥çº¿æ•°æ®å·²ä¿å­˜: {csv_daily_path}\")\n",
    "print(f\"   æ–‡ä»¶å¤§å°: {os.path.getsize(csv_daily_path) / 1024:.2f} KB\")\n",
    "\n",
    "df_basic.to_csv(csv_basic_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nâœ… Basicæ•°æ®å·²ä¿å­˜: {csv_basic_path}\")\n",
    "print(f\"   æ–‡ä»¶å¤§å°: {os.path.getsize(csv_basic_path) / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ‰ æ‰€æœ‰æ•°æ®ä¿å­˜å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æµ‹è¯•è¯»å–ä¿å­˜çš„æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•è¯»å–ä¿å­˜çš„æ•°æ®\n",
    "test_df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"âœ… æ•°æ®è¯»å–æˆåŠŸï¼\")\n",
    "print(f\"   è®°å½•æ•°: {len(test_df)}\")\n",
    "print(f\"   åˆ—æ•°: {len(test_df.columns)}\")\n",
    "print(\"\\nå‰5æ¡è®°å½•:\")\n",
    "display(test_df.head())\n",
    "print(\"\\nå5æ¡è®°å½•:\")\n",
    "display(test_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ•°æ®è¯´æ˜\n",
    "\n",
    "### ä¿å­˜çš„æ–‡ä»¶\n",
    "\n",
    "1. **`data/000155_SZ_full_history.csv`** - å®Œæ•´æ•°æ®ï¼ˆæ—¥çº¿ + basic åˆå¹¶ï¼‰\n",
    "2. **`data/000155_SZ_daily.csv`** - ä»…æ—¥çº¿æ•°æ®\n",
    "3. **`data/000155_SZ_basic.csv`** - ä»… basic æ•°æ®\n",
    "\n",
    "### ä¸»è¦å­—æ®µè¯´æ˜\n",
    "\n",
    "#### æ—¥çº¿æ•°æ®å­—æ®µï¼š\n",
    "- `ts_code`: è‚¡ç¥¨ä»£ç \n",
    "- `trade_date`: äº¤æ˜“æ—¥æœŸ\n",
    "- `open`: å¼€ç›˜ä»·\n",
    "- `high`: æœ€é«˜ä»·\n",
    "- `low`: æœ€ä½ä»·\n",
    "- `close`: æ”¶ç›˜ä»·\n",
    "- `pre_close`: æ˜¨æ”¶ä»·\n",
    "- `change`: æ¶¨è·Œé¢\n",
    "- `pct_chg`: æ¶¨è·Œå¹… (%)\n",
    "- `vol`: æˆäº¤é‡ï¼ˆæ‰‹ï¼‰\n",
    "- `amount`: æˆäº¤é¢ï¼ˆåƒå…ƒï¼‰\n",
    "\n",
    "#### Basic æ•°æ®å­—æ®µï¼š\n",
    "- `turnover_rate`: æ¢æ‰‹ç‡ (%)\n",
    "- `turnover_rate_f`: æ¢æ‰‹ç‡ï¼ˆè‡ªç”±æµé€šè‚¡ï¼‰ (%)\n",
    "- `volume_ratio`: é‡æ¯”\n",
    "- `pe`: å¸‚ç›ˆç‡ï¼ˆæ€»å¸‚å€¼/å‡€åˆ©æ¶¦ï¼‰\n",
    "- `pe_ttm`: å¸‚ç›ˆç‡TTM\n",
    "- `pb`: å¸‚å‡€ç‡ï¼ˆæ€»å¸‚å€¼/å‡€èµ„äº§ï¼‰\n",
    "- `ps`: å¸‚é”€ç‡\n",
    "- `ps_ttm`: å¸‚é”€ç‡TTM\n",
    "- `dv_ratio`: è‚¡æ¯ç‡ (%)\n",
    "- `dv_ttm`: è‚¡æ¯ç‡TTM (%)\n",
    "- `total_share`: æ€»è‚¡æœ¬ï¼ˆä¸‡è‚¡ï¼‰\n",
    "- `float_share`: æµé€šè‚¡æœ¬ï¼ˆä¸‡è‚¡ï¼‰\n",
    "- `free_share`: è‡ªç”±æµé€šè‚¡æœ¬ï¼ˆä¸‡è‚¡ï¼‰\n",
    "- `total_mv`: æ€»å¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰\n",
    "- `circ_mv`: æµé€šå¸‚å€¼ï¼ˆä¸‡å…ƒï¼‰\n",
    "\n",
    "### åç»­ä½¿ç”¨\n",
    "\n",
    "```python\n",
    "# è¯»å–å®Œæ•´æ•°æ®\n",
    "df = pd.read_csv('data/000155_SZ_full_history.csv')\n",
    "\n",
    "# è½¬æ¢æ—¥æœŸæ ¼å¼\n",
    "df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "\n",
    "# æŒ‰æ—¥æœŸæ’åº\n",
    "df = df.sort_values('trade_date')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}